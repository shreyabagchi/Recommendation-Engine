{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('Recommender').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = spark.read.json(\"metaBooks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.select(\"asin\",\"title\",\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      asin|               title|         description|\n",
      "+----------+--------------------+--------------------+\n",
      "|0001048791|The Crucible: Per...|                null|\n",
      "|0001048775|Measure for Measu...|William Shakespea...|\n",
      "|0001048236|The Sherlock Holm...|&#34;One thing is...|\n",
      "|0000401048|The rogue of publ...|                null|\n",
      "|0001019880|Classic Soul Winn...|                null|\n",
      "|0001048813|Archer Christmas ...|                null|\n",
      "|0001148427| Sonatas - For Piano|                null|\n",
      "|0001057170|Classic Connolly ...|[Editor's Note: T...|\n",
      "|0001047566|       Hand in Glove|                null|\n",
      "|0001053396|War Poems: An Ant...|Writing poetry ha...|\n",
      "|0000913154|The Way Things Wo...|                null|\n",
      "|0001072986|As You Like it: C...|William Shakespea...|\n",
      "|0001053744| Pearl and Sir Orfeo|While many reader...|\n",
      "|0001055178|      Master Georgie|Beryl Bainbridge ...|\n",
      "|0001064487|Celebremos Su Gloria|                null|\n",
      "|0001042335|Hamlet: Complete ...|William Shakespea...|\n",
      "|0000202010|The Laurel &amp; ...|                null|\n",
      "|0001057138|Classic Connolly ...|                null|\n",
      "|000136118X|One, Two, Guess Who?|One, Two, Guess W...|\n",
      "|0000000868|Foot Rot of Piper...|                null|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.fillna(\"\",subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.fillna(\"\",subset=['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      asin|               title|         description|\n",
      "+----------+--------------------+--------------------+\n",
      "|0001048791|The Crucible: Per...|                    |\n",
      "|0001048775|Measure for Measu...|William Shakespea...|\n",
      "|0001048236|The Sherlock Holm...|&#34;One thing is...|\n",
      "|0000401048|The rogue of publ...|                    |\n",
      "|0001019880|Classic Soul Winn...|                    |\n",
      "|0001048813|Archer Christmas ...|                    |\n",
      "|0001148427| Sonatas - For Piano|                    |\n",
      "|0001057170|Classic Connolly ...|[Editor's Note: T...|\n",
      "|0001047566|       Hand in Glove|                    |\n",
      "|0001053396|War Poems: An Ant...|Writing poetry ha...|\n",
      "|0000913154|The Way Things Wo...|                    |\n",
      "|0001072986|As You Like it: C...|William Shakespea...|\n",
      "|0001053744| Pearl and Sir Orfeo|While many reader...|\n",
      "|0001055178|      Master Georgie|Beryl Bainbridge ...|\n",
      "|0001064487|Celebremos Su Gloria|                    |\n",
      "|0001042335|Hamlet: Complete ...|William Shakespea...|\n",
      "|0000202010|The Laurel &amp; ...|                    |\n",
      "|0001057138|Classic Connolly ...|                    |\n",
      "|000136118X|One, Two, Guess Who?|One, Two, Guess W...|\n",
      "|0000000868|Foot Rot of Piper...|                    |\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = df_books.withColumn(\"combined\",f.concat(f.col('title'),f.lit(' '), f.col('description')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|      asin|               title|         description|            combined|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|0001048791|The Crucible: Per...|                    |The Crucible: Per...|\n",
      "|0001048775|Measure for Measu...|William Shakespea...|Measure for Measu...|\n",
      "|0001048236|The Sherlock Holm...|&#34;One thing is...|The Sherlock Holm...|\n",
      "|0000401048|The rogue of publ...|                    |The rogue of publ...|\n",
      "|0001019880|Classic Soul Winn...|                    |Classic Soul Winn...|\n",
      "|0001048813|Archer Christmas ...|                    |Archer Christmas ...|\n",
      "|0001148427| Sonatas - For Piano|                    |Sonatas - For Piano |\n",
      "|0001057170|Classic Connolly ...|[Editor's Note: T...|Classic Connolly ...|\n",
      "|0001047566|       Hand in Glove|                    |      Hand in Glove |\n",
      "|0001053396|War Poems: An Ant...|Writing poetry ha...|War Poems: An Ant...|\n",
      "|0000913154|The Way Things Wo...|                    |The Way Things Wo...|\n",
      "|0001072986|As You Like it: C...|William Shakespea...|As You Like it: C...|\n",
      "|0001053744| Pearl and Sir Orfeo|While many reader...|Pearl and Sir Orf...|\n",
      "|0001055178|      Master Georgie|Beryl Bainbridge ...|Master Georgie Be...|\n",
      "|0001064487|Celebremos Su Gloria|                    |Celebremos Su Glo...|\n",
      "|0001042335|Hamlet: Complete ...|William Shakespea...|Hamlet: Complete ...|\n",
      "|0000202010|The Laurel &amp; ...|                    |The Laurel &amp; ...|\n",
      "|0001057138|Classic Connolly ...|                    |Classic Connolly ...|\n",
      "|000136118X|One, Two, Guess Who?|One, Two, Guess W...|One, Two, Guess W...|\n",
      "|0000000868|Foot Rot of Piper...|                    |Foot Rot of Piper...|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re as re\n",
    "from pyspark.ml.feature import CountVectorizer , IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_rdd = df_books.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = df_books_rdd.map(lambda x: x['combined']).filter(lambda y: y is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = book_text                                                   \\\n",
    "    .map( lambda document: document.strip().lower())               \\\n",
    "    .map( lambda document: re.split(\" \", document))          \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "    .map( lambda word: [x for x in word if x not in StopWords])    \\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txts = spark.createDataFrame(tokens, [\"list_of_words\",'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       list_of_words|index|\n",
      "+--------------------+-----+\n",
      "|[performed, stuar...|    0|\n",
      "|[measure, complet...|    1|\n",
      "|[sherlock, holmes...|    2|\n",
      "|[rogue, confessio...|    3|\n",
      "|[classic, soul, t...|    4|\n",
      "|[archer, christma...|    5|\n",
      "|    [sonatas, piano]|    6|\n",
      "|[classic, connoll...|    7|\n",
      "|       [hand, glove]|    8|\n",
      "|[anthology, poetr...|    9|\n",
      "|[things, illustra...|   10|\n",
      "|[like, complete, ...|   11|\n",
      "|[pearl, orfeo, ma...|   12|\n",
      "|[master, georgie,...|   13|\n",
      "|[celebremos, gloria]|   14|\n",
      "|[complete, unabri...|   15|\n",
      "|[laurel, hardy, b...|   16|\n",
      "|[classic, connoll...|   17|\n",
      "|[guess, guess, co...|   18|\n",
      "|       [foot, piper]|   19|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=10.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "result_cv = cvmodel.transform(df_txts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"list_of_words\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|       list_of_words|index|        raw_features|\n",
      "+--------------------+-----+--------------------+\n",
      "|[performed, stuar...|    0|(5000,[1338,2461,...|\n",
      "|[measure, complet...|    1|(5000,[14,124,453...|\n",
      "|[sherlock, holmes...|    2|(5000,[0,8,13,19,...|\n",
      "|[rogue, confessio...|    3|(5000,[1145,4182]...|\n",
      "|[classic, soul, t...|    4|(5000,[191,364,79...|\n",
      "|[archer, christma...|    5|(5000,[697,2773],...|\n",
      "|    [sonatas, piano]|    6| (5000,[1488],[1.0])|\n",
      "|[classic, connoll...|    7|(5000,[4,5,13,25,...|\n",
      "|       [hand, glove]|    8|  (5000,[818],[1.0])|\n",
      "|[anthology, poetr...|    9|(5000,[14,15,23,4...|\n",
      "|[things, illustra...|   10|(5000,[179,300,35...|\n",
      "|[like, complete, ...|   11|(5000,[14,20,124,...|\n",
      "|[pearl, orfeo, ma...|   12|(5000,[6,13,15,30...|\n",
      "|[master, georgie,...|   13|(5000,[4,5,8,10,1...|\n",
      "|[celebremos, gloria]|   14|        (5000,[],[])|\n",
      "|[complete, unabri...|   15|(5000,[14,124,453...|\n",
      "|[laurel, hardy, b...|   16|(5000,[0,463],[1....|\n",
      "|[classic, connoll...|   17|(5000,[117,307,36...|\n",
      "|[guess, guess, co...|   18|(5000,[3,8,19,26,...|\n",
      "|       [foot, piper]|   19| (5000,[3623],[1.0])|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_cv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column list_of_words must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually array<string>.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o91.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column list_of_words must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually array<string>.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:43)\r\n\tat org.apache.spark.ml.feature.IDFBase$class.validateAndTransformSchema(IDF.scala:59)\r\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:68)\r\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:98)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:88)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f3ebe951eb13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0midf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"list_of_words\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0midfModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_txts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mresult_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midfModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_txts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column list_of_words must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually array<string>.'"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       list_of_words|index|        raw_features|            features|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|[performed, stuar...|    0|(5000,[1338,2461,...|(5000,[1338,2461,...|\n",
      "|[measure, complet...|    1|(5000,[14,124,453...|(5000,[14,124,453...|\n",
      "|[sherlock, holmes...|    2|(5000,[0,8,13,19,...|(5000,[0,8,13,19,...|\n",
      "|[rogue, confessio...|    3|(5000,[1145,4182]...|(5000,[1145,4182]...|\n",
      "|[classic, soul, t...|    4|(5000,[191,364,79...|(5000,[191,364,79...|\n",
      "|[archer, christma...|    5|(5000,[697,2773],...|(5000,[697,2773],...|\n",
      "|    [sonatas, piano]|    6| (5000,[1488],[1.0])|(5000,[1488],[5.9...|\n",
      "|[classic, connoll...|    7|(5000,[4,5,13,25,...|(5000,[4,5,13,25,...|\n",
      "|       [hand, glove]|    8|  (5000,[818],[1.0])|(5000,[818],[5.25...|\n",
      "|[anthology, poetr...|    9|(5000,[14,15,23,4...|(5000,[14,15,23,4...|\n",
      "|[things, illustra...|   10|(5000,[179,300,35...|(5000,[179,300,35...|\n",
      "|[like, complete, ...|   11|(5000,[14,20,124,...|(5000,[14,20,124,...|\n",
      "|[pearl, orfeo, ma...|   12|(5000,[6,13,15,30...|(5000,[6,13,15,30...|\n",
      "|[master, georgie,...|   13|(5000,[4,5,8,10,1...|(5000,[4,5,8,10,1...|\n",
      "|[celebremos, gloria]|   14|        (5000,[],[])|        (5000,[],[])|\n",
      "|[complete, unabri...|   15|(5000,[14,124,453...|(5000,[14,124,453...|\n",
      "|[laurel, hardy, b...|   16|(5000,[0,463],[1....|(5000,[0,463],[1....|\n",
      "|[classic, connoll...|   17|(5000,[117,307,36...|(5000,[117,307,36...|\n",
      "|[guess, guess, co...|   18|(5000,[3,8,19,26,...|(5000,[3,8,19,26,...|\n",
      "|       [foot, piper]|   19| (5000,[3623],[1.0])|(5000,[3623],[6.7...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tfidf_rdd = result_tfidf.select('index','features').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA.train(result_tfidf_rdd.mapValues(Vectors.fromML).map(list), k=num_topics, maxIterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNumbers = 5  \n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(3) \\\n",
    "          .setFeaturesCol(\"indexedFeatures\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(transformed)\n",
    "\n",
    "cluster = model.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
